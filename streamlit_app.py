import streamlit as st
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
import torch
from PIL import Image
import io

# Set up Streamlit page
st.set_page_config(page_title="Image Captioning App", page_icon="ðŸ“¸")
st.title("ðŸ“¸ Image Captioning with ViT-GPT2")
st.markdown("Upload an image and get a descriptive caption generated by a powerful AI model.")

# Load the model, feature extractor, and tokenizer
@st.cache_resource
def load_model():
    model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
    feature_extractor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
    tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
    return model, feature_extractor, tokenizer

model, feature_extractor, tokenizer = load_model()

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

max_length = 16
num_beams = 4
gen_kwargs = {"max_length": max_length, "num_beams": num_beams}

def predict_caption(image_data):
    """
    Generates a caption for a given image.
    Args:
        image_data: PIL Image object.
    Returns:
        str: The generated caption.
    """
    if image_data.mode != "RGB":
        image_data = image_data.convert(mode="RGB")

    pixel_values = feature_extractor(images=[image_data], return_tensors="pt").pixel_values
    pixel_values = pixel_values.to(device)

    output_ids = model.generate(pixel_values, **gen_kwargs)
    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
    return preds[0].strip()

# File uploader
uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    # Display the uploaded image
    image = Image.open(uploaded_file)
    st.image(image, caption='Uploaded Image.', use_column_width=True)
    st.write("")
    st.write("Generating caption...")

    # Generate and display caption
    with st.spinner('Thinking...'):
        caption = predict_caption(image)
    st.success("Caption Generated!")
    st.info(f"**Caption:** {caption}")

st.markdown("""
---
**How it works:**
This app uses a pre-trained Vision Encoder-Decoder Model (ViT-GPT2) from Hugging Face Transformers to generate captions for images.
The model first encodes the image and then uses a language model (GPT-2) to decode it into a human-readable description.
""")
