import streamlit as st
from transformers import pipeline
import torch
from PIL import Image
import io

# Set up Streamlit page
st.set_page_config(page_title="Image Captioning App", page_icon="ðŸ“¸")
st.title("ðŸ“¸ Image Captioning with ViT-GPT2")
st.markdown("Upload an image and get a descriptive caption generated by a powerful AI model.")

# Load the pipeline
@st.cache_resource
def load_captioner():
    # Use the pipeline for image captioning
    captioner = pipeline("image-to-text", model="nlpconnect/vit-gpt2-image-captioning", device=0 if torch.cuda.is_available() else -1)
    return captioner

captioner = load_captioner()

def predict_caption_pipeline(image_data, max_length=64, num_beams=4):
    """
    Generates a caption for a given image using the Hugging Face pipeline.
    """
    # The pipeline handles image conversion internally
    result = captioner(image_data, max_new_tokens=max_length, num_beams=num_beams)
    return result[0]['generated_text'].strip()


# File uploader
uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    # Display the uploaded image
    image = Image.open(uploaded_file)
    st.image(image, caption='Uploaded Image.', use_column_width=True)
    st.write("")
    st.write("Generating caption...")

    # Generate and display caption
    with st.spinner('Thinking...'):
        caption = predict_caption_pipeline(image)
    st.success("Caption Generated!")
    st.info(f"**Caption:** {caption}")

st.markdown("""
---
**How it works:**
This app uses a pre-trained Vision Encoder-Decoder Model (ViT-GPT2) from Hugging Face Transformers to generate captions for images.
The model first encodes the image and then uses a language model (GPT-2) to decode it into a human-readable description.
""")
