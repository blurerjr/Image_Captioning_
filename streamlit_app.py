import streamlit as st
import torch
from PIL import Image
import io
from transformers import AutoProcessor, BlipForConditionalGeneration

# --- Streamlit Page Configuration ---
st.set_page_config(page_title="BLIP Image Captioning App", page_icon="üìù")
st.title("üìù Image Captioning with BLIP")
st.markdown("Upload an image to get descriptive captions generated by the Salesforce BLIP model.")
st.markdown("---")

# --- Model Loading (Cached for Performance) ---
@st.cache_resource
def load_blip_model():
    """
    Loads the BLIP model and processor, caching them for efficient Streamlit reruns.
    """
    processor = AutoProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    return processor, model

processor, model = load_blip_model()

# --- Device Configuration ---
# Check if CUDA is available, otherwise use CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
st.sidebar.info(f"Running on: **{device.upper()}**")

# --- Caption Generation Function ---
def generate_blip_captions(image_pil, max_length=50, num_beams=4, num_return_sequences=3):
    """
    Generates captions for a given PIL Image using the BLIP model.

    Args:
        image_pil (PIL.Image.Image): The input image.
        max_length (int): The maximum length of the generated captions.
        num_beams (int): The number of beams for beam search decoding.
        num_return_sequences (int): The number of independent sequences to generate.

    Returns:
        tuple: A tuple of generated caption strings.
    """
    if image_pil.mode != "RGB":
        image_pil = image_pil.convert("RGB")

    # Process the image for the model
    # BLIP's AutoProcessor handles both image normalization and tokenization
    inputs = processor(images=image_pil, return_tensors="pt").to(device)

    # Generate captions
    # Note: BLIP uses 'max_new_tokens' instead of 'max_length' for generation length control
    # And it also supports `num_return_sequences` directly.
    output_ids = model.generate(
        **inputs,
        max_new_tokens=max_length,
        num_beams=num_beams,
        num_return_sequences=num_return_sequences,
        early_stopping=True # Stop beam search when all beams have finished
    )

    # Decode the generated IDs to human-readable text
    preds = processor.batch_decode(output_ids, skip_special_tokens=True)
    preds = [pred.strip() for pred in preds]

    return tuple(preds)

# --- Streamlit UI ---
uploaded_file = st.file_uploader("Upload an image...", type=["jpg", "jpeg", "png"])

# Generation parameters in sidebar for user control
st.sidebar.header("Generation Settings")
selected_max_length = st.sidebar.slider("Max Caption Length", min_value=10, max_value=100, value=50, step=5)
selected_num_beams = st.sidebar.slider("Number of Beams", min_value=1, max_value=10, value=4, step=1)
selected_num_return_sequences = st.sidebar.slider("Number of Captions to Generate", min_value=1, max_value=5, value=3, step=1)

if uploaded_file is not None:
    # Display the uploaded image
    image = Image.open(uploaded_file)
    st.image(image, caption='Uploaded Image.', use_column_width=True)
    st.write("")

    if st.button("Generate Captions"):
        st.write("Generating captions...")
        with st.spinner('Thinking... This might take a moment depending on the image and settings.'):
            captions = generate_blip_captions(
                image,
                max_length=selected_max_length,
                num_beams=selected_num_beams,
                num_return_sequences=selected_num_return_sequences
            )
        
        st.success("Captions Generated!")
        st.subheader("Here are the generated captions:")
        for i, caption in enumerate(captions):
            st.write(f"**Caption {i+1}:** {caption}")
else:
    st.info("Upload an image to begin captioning!")

st.markdown("---")
st.markdown("""
This application utilizes the **Salesforce BLIP (Bootstrapping Language-Image Pre-training)** model from Hugging Face Transformers.
BLIP is a powerful model designed for unified vision-language understanding and generation, providing robust image captioning capabilities.
""")
